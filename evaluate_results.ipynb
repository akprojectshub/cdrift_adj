{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from cdrift import evaluation\n",
    "from cdrift.utils.helpers import readCSV_Lists, convertToTimedelta, importLog\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from statistics import mean, harmonic_mean, stdev\n",
    "from scipy.stats import iqr\n",
    "from typing import List\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "LAG_WINDOW = 200\n",
    "MIN_SUPPORT = 1 # Minimum support for latency calculation --> a parameter setting must have at least 3 instances where a true positive was detected before it can be deemed the \"best parameter setting\" for latency calculation\n",
    "\n",
    "CSV_PATH = \"algorithm_results.csv\"\n",
    "OUT_PATH = \"Evaluation_Results\"\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Log Source</th>\n",
       "      <th>Log</th>\n",
       "      <th>Window Size</th>\n",
       "      <th>SW Step Size</th>\n",
       "      <th>Detected Changepoints</th>\n",
       "      <th>Actual Changepoints for Log</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Average Lag</th>\n",
       "      <th>Duration</th>\n",
       "      <th>...</th>\n",
       "      <th>Seconds per Case</th>\n",
       "      <th>Complete-Window Size</th>\n",
       "      <th>Detection-Window Size</th>\n",
       "      <th>Stable Period</th>\n",
       "      <th>P-Value</th>\n",
       "      <th>Min Adaptive Window</th>\n",
       "      <th>Max Adaptive Window</th>\n",
       "      <th>ADWIN Step Size</th>\n",
       "      <th>MRID</th>\n",
       "      <th>Epsilon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bose J</td>\n",
       "      <td>with_noise_20</td>\n",
       "      <td>log_100_1687182078</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[1126]</td>\n",
       "      <td>[1133]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0 days 00:00:24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bose WC</td>\n",
       "      <td>with_noise_20</td>\n",
       "      <td>log_100_1687182078</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[1142]</td>\n",
       "      <td>[1133]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0 days 00:00:26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bose J</td>\n",
       "      <td>with_noise_20</td>\n",
       "      <td>log_100_1687182078</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[1104]</td>\n",
       "      <td>[1133]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0 days 00:00:30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bose WC</td>\n",
       "      <td>with_noise_20</td>\n",
       "      <td>log_100_1687182078</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[1226]</td>\n",
       "      <td>[1133]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0 days 00:00:31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bose J</td>\n",
       "      <td>without_noise</td>\n",
       "      <td>log_100_1687182078</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[1196]</td>\n",
       "      <td>[1133]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0 days 00:00:24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13794</th>\n",
       "      <td>Zheng DBSCAN</td>\n",
       "      <td>with_noise_10</td>\n",
       "      <td>log_9_1687181928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[869.0, 1980.0, 3569.0]</td>\n",
       "      <td>[1135, 1916, 2696, 2989]</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13795</th>\n",
       "      <td>Zheng DBSCAN</td>\n",
       "      <td>with_noise_10</td>\n",
       "      <td>log_9_1687181928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[869.0, 1980.0, 3569.0]</td>\n",
       "      <td>[1135, 1916, 2696, 2989]</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800.0</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13796</th>\n",
       "      <td>Zheng DBSCAN</td>\n",
       "      <td>with_noise_10</td>\n",
       "      <td>log_9_1687181928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[869.0, 1980.0, 3569.0]</td>\n",
       "      <td>[1135, 1916, 2696, 2989]</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800.0</td>\n",
       "      <td>320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13797</th>\n",
       "      <td>Zheng DBSCAN</td>\n",
       "      <td>with_noise_10</td>\n",
       "      <td>log_9_1687181928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[869.0, 1980.0, 3569.0]</td>\n",
       "      <td>[1135, 1916, 2696, 2989]</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800.0</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13798</th>\n",
       "      <td>Zheng DBSCAN</td>\n",
       "      <td>with_noise_10</td>\n",
       "      <td>log_9_1687181928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[869.0, 1980.0, 3569.0]</td>\n",
       "      <td>[1135, 1916, 2696, 2989]</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13799 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Algorithm     Log Source                 Log  Window Size  \\\n",
       "0            Bose J  with_noise_20  log_100_1687182078        100.0   \n",
       "1           Bose WC  with_noise_20  log_100_1687182078        100.0   \n",
       "2            Bose J  with_noise_20  log_100_1687182078        200.0   \n",
       "3           Bose WC  with_noise_20  log_100_1687182078        200.0   \n",
       "4            Bose J  without_noise  log_100_1687182078        300.0   \n",
       "...             ...            ...                 ...          ...   \n",
       "13794  Zheng DBSCAN  with_noise_10    log_9_1687181928          NaN   \n",
       "13795  Zheng DBSCAN  with_noise_10    log_9_1687181928          NaN   \n",
       "13796  Zheng DBSCAN  with_noise_10    log_9_1687181928          NaN   \n",
       "13797  Zheng DBSCAN  with_noise_10    log_9_1687181928          NaN   \n",
       "13798  Zheng DBSCAN  with_noise_10    log_9_1687181928          NaN   \n",
       "\n",
       "       SW Step Size    Detected Changepoints Actual Changepoints for Log  \\\n",
       "0               2.0                   [1126]                      [1133]   \n",
       "1               2.0                   [1142]                      [1133]   \n",
       "2               2.0                   [1104]                      [1133]   \n",
       "3               2.0                   [1226]                      [1133]   \n",
       "4               2.0                   [1196]                      [1133]   \n",
       "...             ...                      ...                         ...   \n",
       "13794           NaN  [869.0, 1980.0, 3569.0]    [1135, 1916, 2696, 2989]   \n",
       "13795           NaN  [869.0, 1980.0, 3569.0]    [1135, 1916, 2696, 2989]   \n",
       "13796           NaN  [869.0, 1980.0, 3569.0]    [1135, 1916, 2696, 2989]   \n",
       "13797           NaN  [869.0, 1980.0, 3569.0]    [1135, 1916, 2696, 2989]   \n",
       "13798           NaN  [869.0, 1980.0, 3569.0]    [1135, 1916, 2696, 2989]   \n",
       "\n",
       "       F1-Score  Average Lag        Duration  ...  Seconds per Case  \\\n",
       "0      1.000000          7.0 0 days 00:00:24  ...          0.010358   \n",
       "1      1.000000          9.0 0 days 00:00:26  ...          0.011023   \n",
       "2      1.000000         29.0 0 days 00:00:30  ...          0.012817   \n",
       "3      1.000000         93.0 0 days 00:00:31  ...          0.013412   \n",
       "4      1.000000         63.0 0 days 00:00:24  ...          0.010453   \n",
       "...         ...          ...             ...  ...               ...   \n",
       "13794  0.285714         64.0 0 days 00:00:00  ...          0.000138   \n",
       "13795  0.285714         64.0 0 days 00:00:00  ...          0.000138   \n",
       "13796  0.285714         64.0 0 days 00:00:00  ...          0.000138   \n",
       "13797  0.285714         64.0 0 days 00:00:00  ...          0.000138   \n",
       "13798  0.285714         64.0 0 days 00:00:00  ...          0.000138   \n",
       "\n",
       "       Complete-Window Size  Detection-Window Size  Stable Period  P-Value  \\\n",
       "0                       NaN                    NaN            NaN      NaN   \n",
       "1                       NaN                    NaN            NaN      NaN   \n",
       "2                       NaN                    NaN            NaN      NaN   \n",
       "3                       NaN                    NaN            NaN      NaN   \n",
       "4                       NaN                    NaN            NaN      NaN   \n",
       "...                     ...                    ...            ...      ...   \n",
       "13794                   NaN                    NaN            NaN      NaN   \n",
       "13795                   NaN                    NaN            NaN      NaN   \n",
       "13796                   NaN                    NaN            NaN      NaN   \n",
       "13797                   NaN                    NaN            NaN      NaN   \n",
       "13798                   NaN                    NaN            NaN      NaN   \n",
       "\n",
       "       Min Adaptive Window  Max Adaptive Window  ADWIN Step Size   MRID  \\\n",
       "0                      NaN                  NaN              NaN    NaN   \n",
       "1                      NaN                  NaN              NaN    NaN   \n",
       "2                      NaN                  NaN              NaN    NaN   \n",
       "3                      NaN                  NaN              NaN    NaN   \n",
       "4                      NaN                  NaN              NaN    NaN   \n",
       "...                    ...                  ...              ...    ...   \n",
       "13794                  NaN                  NaN              NaN  800.0   \n",
       "13795                  NaN                  NaN              NaN  800.0   \n",
       "13796                  NaN                  NaN              NaN  800.0   \n",
       "13797                  NaN                  NaN              NaN  800.0   \n",
       "13798                  NaN                  NaN              NaN  800.0   \n",
       "\n",
       "       Epsilon  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "...        ...  \n",
       "13794    160.0  \n",
       "13795    240.0  \n",
       "13796    320.0  \n",
       "13797    400.0  \n",
       "13798     80.0  \n",
       "\n",
       "[13799 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = readCSV_Lists(CSV_PATH)\n",
    "df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude Martjushev WC (In favor of LCDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Algorithm\"] != \"Martjushev ADWIN WC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename approaches to shorter names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bose J' 'Bose WC' \"Earth Mover's Distance\" 'LCDD' 'Maaradji Runs'\n",
      " 'Martjushev ADWIN J' 'Process Graph Metrics' 'Zheng DBSCAN']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['J-Measure', 'Window Count', 'EMD', 'LCDD', nan, 'ADWIN J', 'PGM',\n",
       "       'RINV'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df[\"Algorithm\"].unique())\n",
    "shorter_names = {\n",
    "    \"Zheng DBSCAN\": \"RINV\",\n",
    "    \"ProDrift\": \"ProDrift\",\n",
    "    \"Bose J\": \"J-Measure\",\n",
    "    \"Bose WC\": \"Window Count\",\n",
    "    \"Earth Mover's Distance\": \"EMD\",\n",
    "    \"Process Graph Metrics\": \"PGM\",\n",
    "    \"Martjushev ADWIN J\": \"ADWIN J\",\n",
    "    \"Martjushev ADWIN WC\": \"ADWIN WC\",\n",
    "    \"LCDD\": \"LCDD\"\n",
    "}\n",
    "df[\"Algorithm\"] = df[\"Algorithm\"].map(shorter_names)\n",
    "df[\"Algorithm\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Order in which to plot the approaches, to stay consistent\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m ALPHA_SORT_NAMES \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msorted\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mAlgorithm\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m ALPHA_SORT_NAMES\n",
      "\u001B[1;31mTypeError\u001B[0m: '<' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "# Order in which to plot the approaches, to stay consistent\n",
    "ALPHA_SORT_NAMES = sorted(list(df[\"Algorithm\"].unique()))\n",
    "ALPHA_SORT_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only use Atomic Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_logs = {\n",
    "    log\n",
    "    for index, (source, log) in df[[\"Log Source\", \"Log\"]].iterrows()\n",
    "    if (source.lower() == \"ostovar\" and log.lower().startswith(\"atomic\")) # Only use atomic logs from ostovar\n",
    "    or (log.lower().startswith(\"bose\")) # Use bose log\n",
    "    or (source.lower() == \"ceravolo\" and set(log.lower().split(\"_\")[-1]) != {'i', 'o', 'r'}) # Exclude all composite (IOR, ROI, etc.) logs from ceravolo\n",
    "}\n",
    "\n",
    "df = df[df[\"Log\"].isin(atomic_logs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Log Lengths for Scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_groupkeys = list(df.groupby([\"Log Source\", \"Log\"]).groups.keys())\n",
    "_logpaths = [Path(\"EvaluationLogs\", source, f\"{log}.xes.gz\") for source, log in _groupkeys]\n",
    "\n",
    "loglengths = dict()\n",
    "loglengths_events = dict()\n",
    "\n",
    "for logpath in _logpaths:\n",
    "    log = importLog(logpath.as_posix(), verbose=False)\n",
    "    loglengths[logpath] = len(log)\n",
    "    loglengths_events[logpath] = len([evt for case in log for evt in case])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Logs: 0\n",
      "Number of Cases: 0\n",
      "Number of Events: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Logs: {len(loglengths.keys())}\")\n",
    "print(f\"Number of Cases: {sum(loglengths.values())}\")\n",
    "print(f\"Number of Events: {sum(loglengths_events.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of Drifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_drifts = sum(\n",
    "    len(group.iloc[0][\"Actual Changepoints for Log\"])\n",
    "    for name, group in df.groupby([\"Log Source\", \"Log\"])\n",
    ")\n",
    "print(\"Total number of drifts:\", num_drifts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Duplicate Rows (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_name(df):\n",
    "    return [\n",
    "        (alg, df[df[\"Algorithm\"] == alg].copy())\n",
    "        for alg in df[\"Algorithm\"].unique()\n",
    "    ]\n",
    "\n",
    "dfs = split_by_name(df)\n",
    "\n",
    "if not pd.read_csv(CSV_PATH).duplicated().any():\n",
    "    print(\"No duplicate rows in the Dataframe! :)\")\n",
    "else:\n",
    "    for name, d in split_by_name(pd.read_csv(CSV_PATH)):\n",
    "        print(f\"{name} contains duplicate rows: {d.duplicated().any()}\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Auxiliary Columns for Noise and Change Pattern\n",
    "- One column stating the change pattern used in this event log\n",
    "- One column stating the noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_ostovar_to_shortnames = {\n",
    "    \"ConditionalMove\": 'cm',\n",
    "    \"ConditionalRemoval\": 'cre',\n",
    "    \"ConditionalToSequence\": 'cf',\n",
    "    \"Frequency\": 'fr',\n",
    "    \"Loop\": 'lp',\n",
    "    \"ParallelMove\": 'pm',\n",
    "    \"ParallelRemoval\": 'pre',\n",
    "    \"ParallelToSequence\": 'pl',\n",
    "    \"SerialMove\": 'sm',\n",
    "    \"SerialRemoval\": 'sre',\n",
    "    \"Skip\": 'cb',\n",
    "    \"Substitute\": 'rp',\n",
    "    \"Swap\": 'sw',\n",
    "}\n",
    "\n",
    "def map_row_to_cp(row):\n",
    "    logname = row[\"Log\"]\n",
    "    if row[\"Log Source\"] == \"Ceravolo\":\n",
    "        return logname.split(\"_\")[-1]\n",
    "    elif row[\"Log Source\"] == \"Ostovar\":\n",
    "        change_pattern = logname.split(\"_\")[-1]\n",
    "        if change_pattern.isnumeric(): # This log has a noise level; Use the second to last string instead\n",
    "            change_pattern = logname.split(\"_\")[-2]\n",
    "        return mapping_ostovar_to_shortnames[change_pattern]\n",
    "    elif row[\"Log Source\"] == \"Bose\":\n",
    "        return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown Log Source for Versatility: {row['Log Source']}; Is this Log meant to be used for a Versatility Evaluation?\")\n",
    "\n",
    "# Add a column stating the change pattern that is applied\n",
    "df[\"Change Pattern\"] = df.apply(lambda x: map_row_to_cp(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing; Add \"Noise Level\" Column\n",
    "def map_row_to_noise(row):\n",
    "    logname = row[\"Log\"]\n",
    "    if row[\"Log Source\"] == \"Ceravolo\":\n",
    "        return re.search('noise([0-9]*)_', logname).group(1)\n",
    "    elif row[\"Log Source\"] == \"Ostovar\":\n",
    "        # return mapping_ostovar_to_shortnames[logname.split(\"_\")[-1]]\n",
    "        last_member = logname.split('_')[-1]\n",
    "        if last_member.isnumeric():\n",
    "            return last_member\n",
    "        else:\n",
    "            return '0'\n",
    "    elif row[\"Log Source\"] == \"Bose\":\n",
    "        return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown Log Source for Noise Level: {row['Log Source']}; Is this Log meant to be used for a Robustness Evaluation?\")\n",
    "\n",
    "# Only use the Ceravolo and Ostovar Logs as Bose has only 1 log, i.e., no noise levels\n",
    "# Add a column stating the change pattern that is applied and one for the noise level\n",
    "df[\"Noise Level\"] = df.apply(lambda x: map_row_to_noise(x), axis=1)\n",
    "df[['Algorithm', 'Log Source', 'Log', 'Change Pattern', 'Noise Level']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Noisy/Noiseful Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = zip(df[\"Log Source\"], df[\"Log\"])\n",
    "\n",
    "noiseless_logs = {\n",
    "    log for source, log in logs if \n",
    "    (source == \"Ostovar\"  and not (log.endswith(\"_2\") or log.endswith(\"_5\"))) or\n",
    "    (source == \"Ceravolo\" and log.split(\"_\")[2]==\"noise0\") or\n",
    "    source == \"Bose\"\n",
    "}\n",
    "\n",
    "df_noiseless = df[df[\"Log\"].isin(noiseless_logs)]\n",
    "df_noisy = df[df[\"Log\"].isin(noiseless_logs) == False]\n",
    "\n",
    "dfs_noisy = split_by_name(df_noisy)\n",
    "dfs_noiseless = split_by_name(df_noiseless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Color Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#DB444B\",\"#9A607F\",\"#006BA2\",\"#3EBCD2\",\"#379A8B\",\"#EBB434\",\"#B4BA39\",\"#D1B07C\"]\n",
    "color_map = {ALPHA_SORT_NAMES[i]:colors[i] for i in range(0,len(ALPHA_SORT_NAMES))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Change Pattern Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Change Pattern\"].unique()) # None means Bose\n",
    "\n",
    "cp_count_results = { cp: dict() for cp in df[\"Change Pattern\"].unique() if cp is not None } # Bose gets lost because it maps to none\n",
    "\n",
    "for name, group in df[[\"Log Source\", \"Log\", \"Change Pattern\"]].drop_duplicates(inplace=False).groupby(\"Log Source\"): # Only consider each logpath once (not once for each algorithm application)\n",
    "    series = group[\"Change Pattern\"].value_counts().to_dict()\n",
    "    for cp, count in series.items():\n",
    "        cp_count_results[cp][name] = count\n",
    "\n",
    "for key in cp_count_results.keys():\n",
    "    cp_count_results[key][\"Bose\"] = 0\n",
    "cp_count_results[\"Mixed (Bose)\"] = {\"Ostovar\": 0, \"Ceravolo\": 0, \"Bose\": 1}\n",
    "\n",
    "    \n",
    "cp_counts = pd.DataFrame(cp_count_results).fillna(0).astype(int)\n",
    "cp_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Relative Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_counts_tr = cp_counts.transpose()\n",
    "total_logs = cp_counts.to_numpy().sum()\n",
    "cp_counts_tr[\"Relative Frequency\"] = cp_counts_tr.apply(lambda row: row.sum() / total_logs * 100, axis=1)\n",
    "cp_counts_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "- All Change-Patterns\n",
    "- Only 0 Noise\n",
    "- Max cummulative *Recall* achieved of all parameter settings\n",
    "  - Accumulate TP and FP over all event logs before calculating recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_parameters = {\n",
    "        \"Bose J\": [\"Window Size\", \"SW Step Size\"],\n",
    "        \"Bose WC\": [\"Window Size\", \"SW Step Size\"],\n",
    "        \"Martjushev ADWIN J\": [\"Min Adaptive Window\", \"Max Adaptive Window\", \"P-Value\", \"ADWIN Step Size\"],\n",
    "        \"Martjushev ADWIN WC\": [\"Min Adaptive Window\", \"Max Adaptive Window\", \"P-Value\", \"ADWIN Step Size\"],\n",
    "        \"ProDrift\": [\"Window Size\", \"SW Step Size\"],\n",
    "        \"Earth Mover's Distance\": [\"Window Size\", \"SW Step Size\"],\n",
    "        \"Process Graph Metrics\": [\"Min Adaptive Window\", \"Max Adaptive Window\", \"P-Value\"],\n",
    "        \"Zheng DBSCAN\": [\"MRID\", \"Epsilon\"],\n",
    "        \"LCDD\": [\"Complete-Window Size\", \"Detection-Window Size\", \"Stable Period\"]\n",
    "    }\n",
    "\n",
    "used_parameters = {\n",
    "    shorter_names[name]: used_parameters[name]\n",
    "    for name in used_parameters.keys()\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def calcAccuracy(df:pd.DataFrame, param_names:List[str], lag_window: int):\n",
    "    \"\"\"Calculates the Accuracy Metric for the given dataframe by grouping by the given parameters and calculating the mean accuracy\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing the results to be evaluated\n",
    "        param_names (List[str]): The names of the parameters of this approach\n",
    "        lag_window (int): The lag window to be used for the evaluation to determine true positives and false positives\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    f1s = dict()\n",
    "    recalls = dict()\n",
    "    precisions = dict()\n",
    "    # Group by parameter values to calculate accuracy per parameter setting, over all logs\n",
    "    for parameters, group in df.groupby(by=param_names):\n",
    "        # Calculate Accuracy for this parameter setting\n",
    "        ## --> F1-Score, but first collect all TP and FP\n",
    "        tps = 0\n",
    "        fps = 0\n",
    "        positives = 0\n",
    "        detected = 0\n",
    "\n",
    "        # Collect TP FP, etc. \n",
    "        for index, row in group.iterrows():\n",
    "            actual_cp = row[\"Actual Changepoints for Log\"]\n",
    "            detected_cp = row[\"Detected Changepoints\"]\n",
    "            tp, fp = evaluation.getTP_FP(detected_cp, actual_cp, lag_window)\n",
    "            tps += tp\n",
    "            fps += fp\n",
    "            positives += len(actual_cp)\n",
    "            detected += len(detected_cp)\n",
    "\n",
    "        try:\n",
    "            precisions[parameters] = tps / detected\n",
    "        except ZeroDivisionError:\n",
    "            precisions[parameters] = np.NaN\n",
    "        \n",
    "        try:\n",
    "            recalls[parameters] = tps / positives\n",
    "        except ZeroDivisionError:\n",
    "            recalls[parameters] = np.NaN\n",
    "\n",
    "        f1s[parameters] = harmonic_mean([precisions[parameters], recalls[parameters]]) # If either is nan, the harmonic mean is nan\n",
    "    return (precisions, recalls, f1s)\n",
    "\n",
    "def calculate_accuracy_metric_df(dataframe, lag_window, verbose=True):\n",
    "    computed_accuracy_dicts = dict()\n",
    "    computed_precision_dicts = dict()\n",
    "    computed_recall_dicts = dict()\n",
    "\n",
    "    accuracy_best_param = dict()\n",
    "\n",
    "    accuracies = dict()\n",
    "    for name, a_df in dataframe.groupby(by=\"Algorithm\"):\n",
    "        computed_precision_dicts[name], computed_recall_dicts[name], computed_accuracy_dicts[name] = calcAccuracy(a_df, used_parameters[name], lag_window)\n",
    "\n",
    "        best_param = max(computed_accuracy_dicts[name],  key=lambda x: computed_accuracy_dicts[name][x])\n",
    "\n",
    "        accuracy_best_param[name] = best_param\n",
    "\n",
    "        # accuracies[name] = max(computed_accuracy_dicts[name].values())\n",
    "        accuracies[name] = computed_accuracy_dicts[name][best_param]\n",
    "        if verbose:\n",
    "            print(f\"{name}: {accuracies[name]}\")\n",
    "\n",
    "    return (accuracies, computed_accuracy_dicts, computed_precision_dicts, computed_recall_dicts, accuracy_best_param)\n",
    "\n",
    "accuracies, computed_accuracy_dicts, computed_precision_dicts, computed_recall_dicts, accuracy_best_param = calculate_accuracy_metric_df(df_noiseless, LAG_WINDOW, verbose=False)\n",
    "\n",
    "pd.DataFrame([{'Algorithm': name, 'Accuracy': accuracies[name]} for name in accuracies.keys()]).sort_values(by=\"Algorithm\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"Algorithm\"].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Accuracy as Barplot of Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy_plot_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"Algorithm\": name,\n",
    "            \"Metric\": \"F1-Score\",\n",
    "            \"Value\": computed_accuracy_dicts[name][accuracy_best_param[name]]\n",
    "        }\n",
    "        for name in computed_accuracy_dicts.keys()\n",
    "    ] + [\n",
    "        {\n",
    "            \"Algorithm\": name,\n",
    "            \"Metric\": \"Precision\",\n",
    "            \"Value\": computed_precision_dicts[name][accuracy_best_param[name]]\n",
    "        }\n",
    "        for name in computed_accuracy_dicts.keys()\n",
    "    ] + [\n",
    "        {\n",
    "            \"Algorithm\": name,\n",
    "            \"Metric\": \"Recall\",\n",
    "            \"Value\": computed_recall_dicts[name][accuracy_best_param[name]]\n",
    "        }\n",
    "        for name in computed_accuracy_dicts.keys()\n",
    "    ]\n",
    ")\n",
    "\n",
    "palette=None #{\"Precision\": \"#573deb\", \"F1-Score\": \"#ff0076\", \"Recall\": \"#ffa600\"}\n",
    "palette = sns.color_palette(colors)\n",
    "plt.grid(zorder=0)\n",
    "ax = sns.barplot(x=\"Metric\", y=\"Value\", data=accuracy_plot_df, hue=\"Algorithm\", palette=palette, hue_order=ALPHA_SORT_NAMES,zorder =5)\n",
    "\n",
    "ax.figure.set_size_inches(11, 4)\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "ax.tick_params(labelsize=18)\n",
    "ax.set_xlabel(ax.get_xlabel(), size = 20)\n",
    "ax.set_ylabel(\"\", size = 20)\n",
    "#ax.get_yaxis().set_visible(False)\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0,prop={'size': 14})\n",
    "\n",
    "if not os.path.exists(f\"{OUT_PATH}/Accuracy{LAG_WINDOW}\"):\n",
    "    os.makedirs(f\"{OUT_PATH}/Accuracy{LAG_WINDOW}\")\n",
    "plt.savefig(f\"{OUT_PATH}/Accuracy{LAG_WINDOW}/accuracy_c.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot as Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pointplot(\n",
    "    x=\"Algorithm\",\n",
    "    y=\"Value\",\n",
    "    data=accuracy_plot_df,\n",
    "    hue=\"Metric\",\n",
    "    order=ALPHA_SORT_NAMES,\n",
    "    join=True,\n",
    ")\n",
    "ax.figure.set_size_inches(15, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency\n",
    "- Using best parameter setting\n",
    "- Same logs as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def calcLatencies(df, param_names, lag_window):\n",
    "    latencies = dict()\n",
    "    for parameters, group in df.groupby(by=param_names):\n",
    "        lags = []\n",
    "        for index, row in group.iterrows():\n",
    "            actual_cp = row[\"Actual Changepoints for Log\"]\n",
    "            detected_cp = row[\"Detected Changepoints\"]\n",
    "            assignments = evaluation.assign_changepoints(detected_cp, actual_cp, lag_window)\n",
    "            for d, a in assignments:\n",
    "                lags.append(abs(d-a))\n",
    "        latencies[parameters] = lags\n",
    "    return latencies\n",
    "\n",
    "def calculate_latency(dataframe, lag_window, min_support=1, verbose=True):\n",
    "    latencies = dict() # Dict holding the best achieved latency per approach\n",
    "    scaled_latency_dicts = dict() # Dict holding the scaled mean latency per approach per parameter setting\n",
    "    computed_latency_dicts = dict() # Dict holding the raw list of detection lags per approach per parameter setting\n",
    "    best_params_latency = dict() # Dict holding the best parameter setting per approach (the one that achieves the best latency)\n",
    "    for name, a_df in dataframe.groupby(by=\"Algorithm\"):\n",
    "        result = calcLatencies(a_df, used_parameters[name], lag_window)\n",
    "        computed_latency_dicts[name] = result\n",
    "        latency_scaled_dict = {\n",
    "            param: 1-(mean(result[param])/lag_window) if len(result[param]) >= min_support else np.NaN\n",
    "            for param in result.keys()\n",
    "        }\n",
    "        scaled_latency_dicts[name] = latency_scaled_dict\n",
    "        best_param = max(latency_scaled_dict,  key=lambda x: latency_scaled_dict[x] if not np.isnan(latency_scaled_dict[x]) else -1)\n",
    "\n",
    "        best_params_latency[name] = best_param\n",
    "        latencies[name] = latency_scaled_dict[best_param]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{name}: {(1-latencies[name])*lag_window} Traces; Score: {latencies[name]}\")\n",
    "    return (latencies, scaled_latency_dicts, computed_latency_dicts, best_params_latency)\n",
    "\n",
    "latencies, scaled_latency_dicts, computed_latency_dicts, best_params_latency = calculate_latency(df_noiseless, LAG_WINDOW, min_support=MIN_SUPPORT, verbose=False)\n",
    "\n",
    "pd.DataFrame([{'Algorithm': name, 'Lag (Traces)': (1-latencies[name])*LAG_WINDOW, 'Lag (Score)': latencies[name]} for name in latencies.keys()]).sort_values(by=\"Algorithm\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Latency\n",
    "- Plotting the observed average latency along with the corresponding standard deviation for the found best parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_plot_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Algorithm\": name,\n",
    "        \"Unscaled Latency\": latency\n",
    "    }\n",
    "    for name in computed_latency_dicts.keys()\n",
    "    for latency in computed_latency_dicts[name][best_params_latency[name]]\n",
    "])\n",
    "\n",
    "ax = plt.subplots(figsize=(17, 4))\n",
    "\n",
    "# Order by latency, ascending (best is left)\n",
    "# order = [name for name, _ in sorted(latencies.items(), key=lambda x: x[1] if not np.isnan(x[1]) else 0, reverse=True)]\n",
    "# print(order)\n",
    "ax = sns.barplot(x=\"Algorithm\", y=\"Unscaled Latency\", data=latency_plot_df,palette= palette, order=ALPHA_SORT_NAMES)\n",
    "plt.ylabel(\"Latency\")# [Traces]\")\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(ax.get_xlabel(), size = 18)\n",
    "ax.set_ylabel(ax.get_ylabel(), size = 18)\n",
    "ax.figure.set_size_inches(14, 4)\n",
    "\n",
    "if not os.path.exists(f\"{OUT_PATH}/Latency{LAG_WINDOW}\"):\n",
    "    os.makedirs(f\"{OUT_PATH}/Latency{LAG_WINDOW}\")\n",
    "plt.savefig(f\"{OUT_PATH}/Latency{LAG_WINDOW}/latency.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use Ceravolo and Ostovar Logs because these are the only ones that have different change patterns.\n",
    "# (Theoretically we could also drop all rows containing a nan in the change pattern column? This would be more general)\n",
    "df_v = df_noiseless[df_noiseless[\"Log Source\"].isin([\"Ceravolo\", \"Ostovar\"])].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation\n",
    "- Recall of all logs of the same pattern, \"concatenated\", i.e., accumulate TP and FP first\n",
    "- Divide this by the number of change patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_versatility(dataframe, lag_window, verbose=True):\n",
    "\n",
    "    versatility_recall_dicts = dict() # Map approach to a dictionary mapping param setting to mean recall over all change patterns\n",
    "    versatilities = dict() # Map approach to versatility score\n",
    "    best_params_versatility = dict() # Map approach to best param setting\n",
    "\n",
    "    for name, group in dataframe.groupby(by=\"Algorithm\"):\n",
    "        recalls_of_this_approach = dict()\n",
    "        for params, params_group in group.groupby(by=used_parameters[name]):\n",
    "            recalls = dict()\n",
    "            for change_pattern, cp_group in params_group.groupby(by=\"Change Pattern\"):\n",
    "                TPS = 0\n",
    "                POSITIVES = 0\n",
    "                # TP / TP+FN = TP / POSTIVES = Recall\n",
    "                for index, row in cp_group.iterrows():\n",
    "                    detected_changepoints = row[\"Detected Changepoints\"]\n",
    "                    actual_changepoints = row[\"Actual Changepoints for Log\"]\n",
    "\n",
    "                    tp, _ = evaluation.getTP_FP(detected_changepoints, actual_changepoints, lag_window)\n",
    "\n",
    "                    TPS += tp\n",
    "                    POSITIVES += len(actual_changepoints)\n",
    "\n",
    "                # Recall of this algorithm for this change pattern:\n",
    "                recall = TPS / POSITIVES if POSITIVES != 0 else np.NaN # Only the case if there are no actual changepoints, which should not be the case\n",
    "                recalls[change_pattern] = recall\n",
    "            recalls_of_this_approach[params] = recalls\n",
    "        versatility_recall_dicts[name] = recalls_of_this_approach\n",
    "        best_param = max(recalls_of_this_approach, key=lambda x: np.nanmean(list(recalls_of_this_approach[x].values()))) # .values() gives us all the recalls for all change patterns on this param setting\n",
    "        best_params_versatility[name] = best_param\n",
    "        versatilities[name] = np.nanmean(list(\n",
    "            recalls_of_this_approach[best_param].values()\n",
    "        ))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{name}: {versatilities[name]}\")\n",
    "        \n",
    "    return versatilities, versatility_recall_dicts, best_params_versatility\n",
    "\n",
    "versatilities, versatility_recall_dicts, best_params_versatility = calc_versatility(df_v, LAG_WINDOW, verbose=False)\n",
    "\n",
    "pd.DataFrame([{'Algorithm': name, 'Versatility': versatilities[name]} for name in versatilities.keys()]).sort_values(by=\"Algorithm\", ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "- Barplot with \"bar\" for each change pattern and approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vers_plot = pd.DataFrame([\n",
    "    {\n",
    "        \"Algorithm\": name,\n",
    "        \"Change Pattern\": cp,\n",
    "        \"Versatility\": versatility_recall_dicts[name][best_params_versatility[name]][cp]\n",
    "    }\n",
    "    for name in versatility_recall_dicts.keys()\n",
    "    for cp in versatility_recall_dicts[name][best_params_versatility[name]].keys()\n",
    "])\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(20, 4))\n",
    "sns.barplot(x=\"Change Pattern\", y=\"Versatility\", data=df_vers_plot, hue=\"Algorithm\", ax=ax, hue_order=ALPHA_SORT_NAMES)\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0)\n",
    "\n",
    "if not os.path.exists(f\"{OUT_PATH}/Versatility{LAG_WINDOW}\"):\n",
    "    os.makedirs(f\"{OUT_PATH}/Versatility{LAG_WINDOW}\")\n",
    "plt.savefig(f\"{OUT_PATH}/Versatility{LAG_WINDOW}/versatility.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_patterns = sorted(list(df_vers_plot[\"Change Pattern\"].unique()))\n",
    "ax = sns.pointplot(\n",
    "    x=\"Change Pattern\",\n",
    "    y=\"Versatility\",\n",
    "    data=df_vers_plot,\n",
    "    hue=\"Algorithm\",\n",
    "    hue_order=ALPHA_SORT_NAMES,\n",
    "    order=change_patterns\n",
    ")\n",
    "ax.figure.set_size_inches(20,4)\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot again, but split in half\n",
    "first_half = change_patterns[:len(change_patterns)//2]\n",
    "second_half = change_patterns[len(change_patterns)//2:]\n",
    "fig,ax = plt.subplots(figsize=(20, 4))\n",
    "sns.barplot(x=\"Change Pattern\", y=\"Versatility\", data=df_vers_plot[df_vers_plot[\"Change Pattern\"].isin(first_half)], hue=\"Algorithm\", order=first_half, ax=ax, hue_order=ALPHA_SORT_NAMES)\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0)\n",
    "\n",
    "plt.savefig(f\"{OUT_PATH}/Versatility{LAG_WINDOW}/versatility_split_1.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2,ax2 = plt.subplots(figsize=(20, 4))\n",
    "sns.barplot(x=\"Change Pattern\", y=\"Versatility\", data=df_vers_plot[df_vers_plot[\"Change Pattern\"].isin(second_half)], hue=\"Algorithm\", order=second_half, ax=ax2, hue_order=ALPHA_SORT_NAMES)\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0)\n",
    "plt.savefig(f\"{OUT_PATH}/Versatility{LAG_WINDOW}/versatility_split_2.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot as Radar Chart (using plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "plotly_default_colors = colors#px.colors.qualitative.Plotly\n",
    "\n",
    "fill_colors = {\n",
    "    name: plotly_default_colors[idx]\n",
    "    for idx,name in enumerate(df_vers_plot[\"Algorithm\"].unique())\n",
    "}\n",
    "\n",
    "# ALL RADAR CHARTS IN ONE PLOT, I DONT LIKE THE LAYOUT THOUGH :/\n",
    "# fig = make_subplots(rows=2, cols=4, specs=[[{'type': 'polar'}]*4]*2, subplot_titles=df_vers_plot[\"Algorithm\"].unique())#, start_cell=\"bottom-left\")\n",
    "# for idx, (alg, dataframe) in enumerate(df_vers_plot.groupby(by=\"Algorithm\")):\n",
    "#     sub_df = df_vers_plot[df_vers_plot[\"Algorithm\"] == alg]\n",
    "#     fig.add_trace(go.Scatterpolar(\n",
    "#         name = alg,\n",
    "#         r=sub_df['Versatility'].values,\n",
    "#         theta=sub_df['Change Pattern'].values,\n",
    "#         fillcolor=fill_colors[alg],\n",
    "#         line={'color': fill_colors[alg]}\n",
    "#     ), row=idx//4+1, col=idx%4+1 )\n",
    "# fig.update_traces(fill='toself')\n",
    "\n",
    "# ALL APPROACHES IN THE SAME RADAR CHART - NOT SO READABLE\n",
    "# fig = go.Figure()\n",
    "# for alg, dataframe in df_vers_plot.groupby(by=\"Algorithm\"):\n",
    "#     fig.add_trace(go.Scatterpolar(\n",
    "#       r = dataframe[\"Versatility\"],\n",
    "#       theta = dataframe[\"Change Pattern\"],\n",
    "#       fill = 'toself',\n",
    "#       name = alg\n",
    "# ))\n",
    "\n",
    "# 1 RADAR CHART PER APPROACH - SEPERATE PLOTS - This is my favorite; Can make a nice layout in latex\n",
    "\n",
    "import warnings\n",
    "# Futurewarnings will be risen by pandas within plotly, but this is plotly's fault, not ours\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    for alg, dataframe in df_vers_plot.groupby(by=\"Algorithm\"):\n",
    "        fig = px.line_polar(\n",
    "            df_vers_plot[df_vers_plot[\"Algorithm\"] == alg],\n",
    "            r='Versatility',\n",
    "            theta='Change Pattern',\n",
    "            line_close=True,\n",
    "            color='Algorithm',\n",
    "            color_discrete_map=fill_colors,\n",
    "            title=None# alg, # Set this to alg if we want the title, I was planning on setting the title in latex\n",
    "        )\n",
    "        fig.update_layout(showlegend=False, title_x=0.5, font=dict(size=18)) # Disable legend (as we have only one algorithm per plot) and center title (if present)\n",
    "        fig.update_traces(fill='toself')\n",
    "\n",
    "        if not os.path.exists(f\"{OUT_PATH}/Versatility{LAG_WINDOW}/Radar_Charts\"):\n",
    "            os.makedirs(f\"{OUT_PATH}/Versatility{LAG_WINDOW}/Radar_Charts\")\n",
    "        fig.write_image(f\"{OUT_PATH}/Versatility{LAG_WINDOW}/Radar_Charts/radar_chart_{alg.replace(' ','_')}.pdf\", format=\"pdf\")\n",
    "        print(alg)\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability\n",
    "- Mean Duration\n",
    "- Across ***all*** logs this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalabilities = dict()\n",
    "for name, a_df in df.groupby(\"Algorithm\"):\n",
    "    result = a_df[\"Duration (Seconds)\"].mean()\n",
    "    result_str = datetime.strftime(datetime.utcfromtimestamp(result), '%H:%M:%S')\n",
    "    scalabilities[name] = {\"avg_seconds\": result, \"str\": result_str}\n",
    "\n",
    "pd.DataFrame([{'Algorithm': name, 'Scalability': scalabilities[name][\"str\"], 'Average Seconds': scalabilities[name]['avg_seconds']} for name in scalabilities.keys()]).sort_values(by=\"Algorithm\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Boxplot of Mean Absolute Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalability_plot_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Algorithm\": name,\n",
    "        \"Duration\": duration / 60\n",
    "    }\n",
    "    for name, a_df in dfs\n",
    "    for duration in a_df[\"Duration (Seconds)\"].tolist()\n",
    "])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.set_xlabel(\"Duration [Minutes]\")\n",
    "\n",
    "sorted_names = list(scalabilities.items())\n",
    "sorted_names.sort(key=lambda x: x[1][\"avg_seconds\"])\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    data=scalability_plot_df,\n",
    "    order=ALPHA_SORT_NAMES,#[name for name, _ in sorted_names] if False else None, # Disable sorting for now, maybe this means that the order is the same in every plot\n",
    "    x=\"Duration\",\n",
    "    y=\"Algorithm\",\n",
    "    palette = palette,\n",
    "    width=1,\n",
    "    ax=ax,\n",
    "    fliersize=0\n",
    ")\n",
    "#ax.figure.set_size_inches(14, 4)\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(ax.get_xlabel(), size = 18)\n",
    "ax.set_ylabel(ax.get_ylabel(), size = 18)\n",
    "plt.xlabel(\"Duration [Minutes]\")\n",
    "\n",
    "if not os.path.exists(f\"{OUT_PATH}/Scalability{LAG_WINDOW}\"):\n",
    "    os.makedirs(f\"{OUT_PATH}/Scalability{LAG_WINDOW}\")\n",
    "plt.savefig(f\"{OUT_PATH}/Scalability{LAG_WINDOW}/scalability.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot as Barplot as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalability_barplot_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Algorithm\": name,\n",
    "        \"Duration\": duration / 60\n",
    "    }\n",
    "    for name, a_df in dfs\n",
    "    for duration in a_df[\"Duration (Seconds)\"].tolist()\n",
    "])\n",
    "\n",
    "ax = plt.subplots(figsize=(17, 4))\n",
    "\n",
    "# Order by latency, ascending (best is left)\n",
    "# order = [name for name, _ in sorted(latencies.items(), key=lambda x: x[1] if not np.isnan(x[1]) else 0, reverse=True)]\n",
    "# print(order)\n",
    "\n",
    "ax = sns.barplot(x=\"Algorithm\", y=\"Duration\", data=scalability_barplot_df,palette = palette, order=ALPHA_SORT_NAMES)\n",
    "ax.figure.set_size_inches(12, 4)\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(ax.get_xlabel(), size = 18)\n",
    "ax.set_ylabel(ax.get_ylabel(), size = 18)\n",
    "plt.ylabel(\"Duration [Minutes]\")\n",
    "plt.savefig(f\"{OUT_PATH}/Scalability{LAG_WINDOW}/scalability_barplot.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seconds per Case/Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seconds = df.copy()\n",
    "# Add Log Length Column\n",
    "df_seconds[\"Log Length (Cases)\"] = df_seconds[[\"Log Source\", \"Log\"]].apply(axis=1, func=lambda x: loglengths[Path(\"EvaluationLogs\", x[0], x[1]+\".xes.gz\")])\n",
    "df_seconds[\"Log Length (Events)\"] = df_seconds[[\"Log Source\", \"Log\"]].apply(axis=1, func=lambda x: loglengths_events[Path(\"EvaluationLogs\", x[0], x[1]+\".xes.gz\")])\n",
    "\n",
    "seconds_per_case = dict()\n",
    "seconds_per_event = dict()\n",
    "\n",
    "for name, group in df_seconds.groupby(\"Algorithm\"):\n",
    "    total_seconds = group[\"Duration (Seconds)\"].sum()\n",
    "\n",
    "    total_cases = group[\"Log Length (Cases)\"].sum()\n",
    "    total_events = group[\"Log Length (Events)\"].sum()\n",
    "\n",
    "    seconds_per_case[name] = total_seconds / total_cases\n",
    "    seconds_per_event[name] = total_seconds / total_events\n",
    "\n",
    "pd.DataFrame([{'Algorithm': name, 'Seconds per Case': seconds_per_case[name], 'Seconds per Event': seconds_per_event[name]} for name in seconds_per_case.keys()]).sort_values(by=\"Algorithm\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seconds[\"Milliseconds per Event\"] = df_seconds.apply(lambda x: (x[\"Duration (Seconds)\"]*1000) / x[\"Log Length (Events)\"], axis=1)\n",
    "df_seconds[[\"Duration (Seconds)\", \"Log Length (Events)\", \"Milliseconds per Event\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Boxplot and Barplot of milliseconds per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.set_xlabel(\"Milliseconds per Event\")\n",
    "\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    data=df_seconds,\n",
    "    order=ALPHA_SORT_NAMES,\n",
    "    x=\"Milliseconds per Event\",\n",
    "    y=\"Algorithm\",\n",
    "    palette = palette,\n",
    "    width=1,\n",
    "    ax=ax,\n",
    "    fliersize=0\n",
    "\n",
    ")\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(ax.get_xlabel(), size = 18)\n",
    "ax.set_ylabel(ax.get_ylabel(), size = 18)\n",
    "plt.xlabel(\"Milliseconds per Event\")\n",
    "\n",
    "if not os.path.exists(f\"{OUT_PATH}/Scalability{LAG_WINDOW}\"):\n",
    "    os.makedirs(f\"{OUT_PATH}/Scalability{LAG_WINDOW}\")\n",
    "plt.savefig(f\"{OUT_PATH}/Scalability{LAG_WINDOW}/scalability_mseconds_per_event.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplots(figsize=(17, 4))\n",
    "\n",
    "ax = sns.barplot(x=\"Algorithm\", y=\"Milliseconds per Event\", data=df_seconds, palette = palette, order=ALPHA_SORT_NAMES)\n",
    "ax.figure.set_size_inches(12, 4)\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(ax.get_xlabel(), size = 18)\n",
    "ax.set_ylabel(ax.get_ylabel(), size = 18)\n",
    "ax.set_yscale(\"log\")\n",
    "plt.ylabel(\"Milliseconds per Event\")\n",
    "\n",
    "if not os.path.exists(f\"{OUT_PATH}/Scalability{LAG_WINDOW}\"):\n",
    "    os.makedirs(f\"{OUT_PATH}/Scalability{LAG_WINDOW}\")\n",
    "plt.savefig(f\"{OUT_PATH}/Scalability{LAG_WINDOW}/scalability_mseconds_per_event_barplot.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Sensitivity\n",
    "- For each Parameter setting, harmonic mean of *accuracy*, *latency*, and *versatility*\n",
    "  - &rarr; Visualize the variance, etc., with a boxplot\n",
    "- Only noiseless logs again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versatility_score_per_param = {\n",
    "    name: {\n",
    "        params: np.nanmean(list(versatility_recall_dicts[name][params].values()))\n",
    "        for params in versatility_recall_dicts[name].keys()\n",
    "    }\n",
    "    for name in versatility_recall_dicts.keys()\n",
    "}\n",
    "\n",
    "\n",
    "sensitivities = dict()\n",
    "for name in df[\"Algorithm\"].unique():\n",
    "    _sensitivities = dict()\n",
    "    acc = computed_accuracy_dicts[name]\n",
    "    lat = scaled_latency_dicts[name]\n",
    "    vers = versatility_score_per_param[name]\n",
    "    for param_choice in acc.keys():\n",
    "        sensitivity = harmonic_mean([acc[param_choice], vers[param_choice], lat[param_choice]])\n",
    "        _sensitivities[param_choice] = sensitivity\n",
    "    sensitivities[name] = _sensitivities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Parameters\": param,\n",
    "        \"Algorithm\": name,\n",
    "        \"Sensitivity Score\": sens\n",
    "    }\n",
    "    for name, sens_dict in sensitivities.items()\n",
    "    for param, sens in sens_dict.items()\n",
    "])\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax = sns.boxplot(\n",
    "    data=sens_df,\n",
    "    x=\"Sensitivity Score\",\n",
    "    y=\"Algorithm\",\n",
    "    palette = palette,\n",
    "    ax=ax,\n",
    "    fliersize=0,\n",
    "    order=ALPHA_SORT_NAMES\n",
    ")\n",
    "ax.figure.set_size_inches(7, 4)\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(\"Performance\", size = 18)\n",
    "ax.set_ylabel(ax.get_ylabel(), size = 18)\n",
    "\n",
    "if not os.path.exists(f\"{OUT_PATH}/Parameter_Sensitivity{LAG_WINDOW}\"):\n",
    "    os.makedirs(f\"{OUT_PATH}/Parameter_Sensitivity{LAG_WINDOW}\")\n",
    "plt.savefig(f\"{OUT_PATH}/Parameter_Sensitivity{LAG_WINDOW}/param_sensitivity.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inter Quartile Range\n",
    "sensitivity_iqrs = dict()\n",
    "for name, sens in sensitivities.items():\n",
    "    _iqr = iqr(list(sens.values()), nan_policy=\"omit\")\n",
    "    sensitivity_iqrs[name] = _iqr\n",
    "\n",
    "pd.DataFrame([{'Algorithm': name, 'Inner-Quartile Range': sensitivity_iqrs[name]} for name in sensitivity_iqrs.keys()]).sort_values(by=\"Algorithm\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only use the Ceravolo and Ostovar Logs as Bose has only 1 log, i.e., no noise levels\n",
    "df_robust = df[df[\"Log Source\"].isin([\"Ceravolo\", \"Ostovar\"])].copy(deep=True)\n",
    "df_robust[['Algorithm', 'Log Source', 'Log', 'Change Pattern', 'Noise Level']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Harmonic Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_robustnesses(dataframe, lag_window):\n",
    "    robustnesses = dict()\n",
    "\n",
    "    for noise_level, noise_df in dataframe.groupby(\"Noise Level\"):\n",
    "        # Calculate accuracy over these logs\n",
    "        accuracies, _, _, _, _ = calculate_accuracy_metric_df(noise_df, lag_window, verbose=False)\n",
    "        latencies, _, _, _ = calculate_latency(noise_df, lag_window, min_support=MIN_SUPPORT, verbose=False)\n",
    "        versatilities, _, _ = calc_versatility(noise_df, lag_window, verbose=False)\n",
    "\n",
    "        assert accuracies.keys() == latencies.keys() == versatilities.keys()\n",
    "\n",
    "        robustnesses[noise_level] = {\n",
    "            approach: harmonic_mean([accuracies[approach], latencies[approach], versatilities[approach]])\n",
    "            for approach in accuracies.keys()\n",
    "        }\n",
    "    return robustnesses\n",
    "\n",
    "means_ceravolo = calc_robustnesses(df_robust[df_robust[\"Log Source\"] == \"Ceravolo\"], LAG_WINDOW)\n",
    "means_ostovar = calc_robustnesses(df_robust[df_robust[\"Log Source\"] == \"Ostovar\"], LAG_WINDOW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_robust = pd.DataFrame(\n",
    "    ([\n",
    "        {\n",
    "            \"Log Set\": \"Ostovar\",\n",
    "            \"Approach\": name,\n",
    "            \"Noise Level\": int(noise_level),\n",
    "            \"Robustness\": robustness\n",
    "        }\n",
    "        for noise_level, robust_dict  in means_ostovar.items()\n",
    "        for name, robustness in robust_dict.items()\n",
    "    ] + [\n",
    "        {\n",
    "            \"Log Set\": \"Ceravolo\",\n",
    "            \"Approach\": name,\n",
    "            \"Noise Level\": int(noise_level),\n",
    "            \"Robustness\": robustness\n",
    "        }\n",
    "        for noise_level, robust_dict  in means_ceravolo.items()\n",
    "        for name, robustness in robust_dict.items()\n",
    "    ])\n",
    ")\n",
    "\n",
    "ax = sns.pointplot(x=\"Noise Level\", y=\"Robustness\", hue=\"Approach\", data=plot_df_robust[plot_df_robust[\"Log Set\"] == \"Ceravolo\"], errorbar=None)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "ax.set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC Thing\n",
    "def convert_harm_mean_to_auc(means, div_zero_default=0):\n",
    "    ## Conventions:\n",
    "        # If any harmonic mean is nan, then set it to 0 instead\n",
    "\n",
    "\n",
    "    def _convert_nan_to_zero(x):\n",
    "        if np.isnan(x):\n",
    "            x = 0\n",
    "        return x\n",
    "\n",
    "    # means maps {noise_level: {approach: robustness}}\n",
    "    # reformat to {approach: [(noise_level, robustness)]}\n",
    "    means = {\n",
    "        approach: [\n",
    "            (int(noise_level), _convert_nan_to_zero(means[noise_level][approach]))\n",
    "            for noise_level in means.keys()\n",
    "        ]\n",
    "        for approach in means[\"0\"].keys()\n",
    "    }\n",
    "\n",
    "    robustnesses = dict()\n",
    "    for approach in means.keys():\n",
    "        points = sorted(means[approach], key=lambda x: x[0])\n",
    "\n",
    "        # Initial Robustness - Use to model \"Ideal Robustness\"\n",
    "        initial_robustness = points[0][1] # Robustness for lowest (0%) noise\n",
    "        ideal_auc = initial_robustness * (points[-1][0] - points[0][0]) # AUC if it was always exactly as good as for the lowest noise level. If performance increases with higher noise, the achieved AUC might be larger than ideal AUC\n",
    "        prev_noise, prev_robust = points[0]\n",
    "        auc = 0\n",
    "        for noise, robust in points[1:]:\n",
    "            delta_noise = noise-prev_noise\n",
    "            sum_robust = prev_robust+robust\n",
    "\n",
    "            # \"ROC Index\"\n",
    "            area = (delta_noise * sum_robust) / 2# Area under curve for this segment\n",
    "            auc += area\n",
    "\n",
    "            prev_noise = noise\n",
    "            prev_robust = robust\n",
    "\n",
    "        if ideal_auc != 0:\n",
    "            robustnesses[approach] = auc / ideal_auc\n",
    "        else:\n",
    "            robustnesses[approach] = div_zero_default\n",
    "    return robustnesses\n",
    "\n",
    "robustness_ceravolo = convert_harm_mean_to_auc(means_ceravolo)\n",
    "robustness_ostovar = convert_harm_mean_to_auc(means_ostovar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Final Robustness Score by Calculating mean of Ceravolo and Ostovar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert robustness_ceravolo.keys() == robustness_ostovar.keys() # This should always be the case..\n",
    "robustnesses = {\n",
    "    name: (robustness_ceravolo[name] + robustness_ostovar[name]) / 2\n",
    "    for name in robustness_ceravolo.keys()\n",
    "}\n",
    "\n",
    "pd.DataFrame([{'Algorithm': name, 'Robustness': robustnesses[name]} for name in robustnesses.keys()]).sort_values(by=\"Algorithm\", ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_robustness(robustness_df, logset:str, color_map):\n",
    "    fig, axs = plt.subplots(2,4, figsize=(12,6))\n",
    "    #fig.suptitle(f\"Robustness of Approaches for {logset} Logs\")\n",
    "    \n",
    "    for idx, name in enumerate(robustness_df[\"Approach\"].unique()):\n",
    "        row_offset = idx // 4\n",
    "        col_offset = idx % 4\n",
    "        ax = axs[row_offset, col_offset]\n",
    "        relevant_df = robustness_df[(robustness_df[\"Approach\"] == name) & (robustness_df[\"Log Set\"] == logset)]\n",
    "        sns.pointplot(x=\"Noise Level\", y=\"Robustness\", data=relevant_df, errorbar=None,color= color_map[name], ax=ax)#, title=name)\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.set_title(name, size = 16)\n",
    "        ax.spines.right.set_visible(False)\n",
    "        ax.spines.top.set_visible(False)\n",
    "        ax.tick_params(labelsize=16)\n",
    "        ax.set_xlabel(ax.get_xlabel() if row_offset == 1 else \"\", size = 18)\n",
    "        ax.set_ylabel(\"Performance\" if col_offset == 0 else \"\", size = 18)\n",
    "        robustness_of_zero_noise = min(zip(relevant_df[\"Noise Level\"], relevant_df[\"Robustness\"]), key=lambda x: x[0])[1]\n",
    "        if np.isnan(robustness_of_zero_noise):\n",
    "            robustness_of_zero_noise = 0\n",
    "        # plt.axhline(y=robustness_of_zero_noise, color='r', linestyle='-', ax=ax)\n",
    "        ax.hlines(robustness_of_zero_noise, color=color_map[name], linestyle='dashed', xmin=0, xmax=len(relevant_df[\"Noise Level\"].unique())-1)\n",
    "        fig.tight_layout()\n",
    "        #ax.grid()\n",
    "        fig.set_facecolor('0.9')\n",
    "    return fig, axs\n",
    "\n",
    "if not os.path.exists(f\"{OUT_PATH}/Robustness{LAG_WINDOW}\"):\n",
    "    os.makedirs(f\"{OUT_PATH}/Robustness{LAG_WINDOW}\")\n",
    "\n",
    "fig_ostovar, axs_ostovar = plot_robustness(plot_df_robust, \"Ostovar\", color_map)\n",
    "fig_ostovar.savefig(f\"{OUT_PATH}/Robustness{LAG_WINDOW}/robustness_ostovar_logs.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show(fig_ostovar)\n",
    "\n",
    "fig_ceravolo, axs_ceravolo = plot_robustness(plot_df_robust, \"Ceravolo\", color_map)\n",
    "fig_ceravolo.savefig(f\"{OUT_PATH}/Robustness{LAG_WINDOW}/robustness_ceravolo_logs.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show(fig_ceravolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_robustness(robustness_df, logset:str, color_map):\n",
    "    \n",
    "    for _, name in enumerate(robustness_df[\"Approach\"].unique()):\n",
    "        relevant_df = robustness_df[(robustness_df[\"Approach\"] == name) & (robustness_df[\"Log Set\"] == logset)]\n",
    "        ax = sns.pointplot(x=\"Noise Level\", y=\"Robustness\", data=relevant_df, errorbar=None,color= color_map[name])#, title=name)\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.figure.set_size_inches(5, 12)\n",
    "        #ax.set_title(logset+\" Logs with Different Noise Levels\", size = 18)\n",
    "        ax.set_xlabel(\"Noise (%)\")\n",
    "        ax.spines.right.set_visible(False)\n",
    "        ax.spines.top.set_visible(False)\n",
    "        ax.tick_params(labelsize=16)\n",
    "        ax.set_xlabel(ax.get_xlabel(), size = 18)\n",
    "        ax.set_ylabel(ax.get_ylabel(), size = 18)\n",
    "        robustness_of_zero_noise = min(zip(relevant_df[\"Noise Level\"], relevant_df[\"Robustness\"]), key=lambda x: x[0])[1]\n",
    "        if np.isnan(robustness_of_zero_noise):\n",
    "            robustness_of_zero_noise = 0\n",
    "        # plt.axhline(y=robustness_of_zero_noise, color='r', linestyle='-', ax=ax)\n",
    "        ax.hlines(robustness_of_zero_noise, color=color_map[name], linestyle='dashed', xmin=0, xmax=len(relevant_df[\"Noise Level\"].unique())-1)\n",
    "    return ax.figure\n",
    "\n",
    "if not os.path.exists(f\"{OUT_PATH}/Robustness{LAG_WINDOW}\"):\n",
    "    os.makedirs(f\"{OUT_PATH}/Robustness{LAG_WINDOW}\")\n",
    "\n",
    "fig_ostovar = plot_robustness(plot_df_robust, \"Ostovar\", color_map)\n",
    "fig_ostovar.savefig(f\"{OUT_PATH}/Robustness{LAG_WINDOW}/robustness_ostovar_logs_all_in_one.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show(fig_ostovar)\n",
    "\n",
    "fig_ceravolo = plot_robustness(plot_df_robust, \"Ceravolo\", color_map)\n",
    "fig_ceravolo.savefig(f\"{OUT_PATH}/Robustness{LAG_WINDOW}/robustness_ceravolo_logs_all_in_one.pdf\", bbox_inches=\"tight\", format=\"pdf\")\n",
    "plt.show(fig_ceravolo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Algorithm\": name,\n",
    "        \"Accuracy\": accuracies[name],\n",
    "        \"Latency\": (1-latencies[name])*LAG_WINDOW,\n",
    "        \"Versatility\": versatilities[name],\n",
    "        \"Scalability\": scalabilities[name][\"str\"],\n",
    "        \"Milliseconds per Case\": seconds_per_case[name]*1000,\n",
    "        \"Milliseconds per Event\": seconds_per_event[name]*1000,\n",
    "        \"Parameter Sensitivity (IQR)\": sensitivity_iqrs[name],\n",
    "        \"Robustness\": robustnesses[name]\n",
    "    }\n",
    "    for name in df[\"Algorithm\"].unique()\n",
    "])\n",
    "\n",
    "results_df.to_csv(f\"{OUT_PATH}/evaluation_measures{LAG_WINDOW}.csv\", index=False)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "3920da902623b129675f574c0b2a6825634412ad9c9c1fca8b4b139be5a4e0fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
